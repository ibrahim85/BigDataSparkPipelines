{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COURSEWORK 2: SPAM CLASSIFICATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This coursework is based on a classification task on the spam dataset. Typically, spam is tackled via Naive Bayes, however this approach will test Logistic Regression as the machine learning classifier. Here, the task is to correctly classify spam documents from documents that are not considered spam. The choice of dataset is the spam dataset, and the choice of machine learning algorithms is due to being a binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "prefix = '/data/tempstore/' # filesystem\n",
    "dirPath = prefix+'spam/bare/part[1-9]' #Call the spam dataset  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Task a) Define a function that reads the rows in the dataframe and creates a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_dataframe_rows( argDir ):  \n",
    "    ft_RDD = sc.wholeTextFiles(argDir) #read files \n",
    "    print('Read {} files from directory {}'.format(ft_RDD.count(),argDir)) \n",
    "    #Create an RDD through a lambda function with an if statement where if \n",
    "    #labelled spmsg then give a label of zero, otherwise all other instances a label of 1.0\n",
    "    RDD_spam = ft_RDD.map(lambda ft: (ft[1], 0.0 if re.search('spmsg',ft[0]) is None else 1.0))\n",
    "    \n",
    "    DataFrame_rows = spark.createDataFrame(RDD_spam, schema=['text','label']) # create a DataFrame with text and labels\n",
    "    \n",
    "    return DataFrame_rows # return function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Test the output of the function by printing out 5 top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2602 files from directory /data/tempstore/spam/bare/part[1-9]\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Subject: becoming...|  0.0|\n",
      "|Subject: zero dow...|  1.0|\n",
      "|Subject: how does...|  1.0|\n",
      "|Subject: philosop...|  0.0|\n",
      "|Subject: job - un...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Testing completed\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "DF_spam = read_dataframe_rows(dirPath)\n",
    "DF_spam.show(5)\n",
    "\n",
    "print(\"Testing completed\") #print when output is completed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Task b) Implement a machine learning pipeline in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import all the relevant libraries.\n",
    "import time \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the training and test datasets with a split of 80% to 20%.\n",
    "dataset = DF_spam\n",
    "tr, te = dataset.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Configure the ML pipeline. We use the tokenizer as the feature transformer and the HashingTF as the feature extractor. We use Logistic Regression and implement all these in the pipeline. The tokenizer breaks up the sentences into individual terms hence being a feature transformer. For feature extraction, HashingTF maps a sequence of terms to their term frequencies via the hashing trick. Finally, logistic regression is implemented as the machine learning classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tokenizer as feature transforming step\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") \n",
    "#HashingTF as the feature extractor step\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") \n",
    "#Choose Logistic regression as the machine learning classifier\n",
    "logRes = LogisticRegression(maxIter=20)\n",
    "\n",
    "#Finally the pipeline argument takes in the tokenizer (feature transforming), feature extracting \n",
    "#and the machine learning algorithm, which in this case is logistic regression.\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, logRes]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, Tasks c) and d) are done together where a grid search is implemented and the performance of the pipeline is evaluated via TrainValidationSplit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A ParamGridBuilder is used to construct a grid search, which takes in arguments about the feature transformers and any parameter changes possible for the machine learning classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100]) \\\n",
    "    .addGrid(logRes.regParam, [0]) \\\n",
    "    .addGrid(logRes.maxIter, [10]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a Binary Classification Evaluator. Logistic regression is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "binary_class_eval=BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TrainValidationSplit will try all combinations of values and determine best model using an evaluator. The pipeline is used as an estimator. We introduce the TrainValidationSplit, which uses an estimator, estimator ParamMaps and an evaluator.\n",
    "\n",
    "In our case, the estimator will use the pipeline, the estimator ParamMaps is essentially the gridsearch and the evaluator is the binary class evaluator. \n",
    "\n",
    "The trainRatio split will be 80% for training and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_val_split = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=binary_class_eval,\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Create a model by fitting the TrainValidationSplit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = train_val_split.fit(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training model is  PipelineModel_4f188a4b744f898e969e\n",
      "best training model results:  0.9944077047982728\n",
      "Time taken is 16.751314163208008\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on training documents.\n",
    "tr_Start_time = time.time() #start timer\n",
    "prediction = model.transform(tr)\n",
    "print(\"The best training model is \",model.bestModel)\n",
    "print(\"best training model results: \", binary_class_eval.evaluate(prediction))\n",
    "tr_End_time = time.time() #end timer\n",
    "tr_TimeTaken = tr_End_time - tr_Start_time\n",
    "print(\"Time taken is\", tr_TimeTaken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best testing model is  PipelineModel_4f188a4b744f898e969e\n",
      "best testing model results:  0.9881434355118566\n",
      "Time taken is 16.725611448287964\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test documents. \n",
    "te_Start_time = time.time() #start timer\n",
    "prediction = model.transform(te)\n",
    "print(\"The best testing model is \", model.bestModel)\n",
    "print(\"best testing model results: \", binary_class_eval.evaluate(prediction))\n",
    "te_End_time = time.time() #end timer\n",
    "te_TimeTaken = te_End_time - te_Start_time\n",
    "print(\"Time taken is\", te_TimeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can make predictions on the test data and show 30 results. The model is basically a combination of all the parameters that performed the best. This is so we can get a better view of the classification process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(100,[0,2,5,10,11...|  0.0|       0.0|\n",
      "|(100,[1,3,7,10,18...|  1.0|       0.0|\n",
      "|(100,[1,2,3,4,5,6...|  1.0|       1.0|\n",
      "|(100,[0,1,3,4,5,6...|  0.0|       0.0|\n",
      "|(100,[0,1,2,4,5,6...|  0.0|       0.0|\n",
      "|(100,[1,6,9,10,11...|  0.0|       0.0|\n",
      "|(100,[0,1,2,4,5,6...|  0.0|       0.0|\n",
      "|(100,[0,1,4,6,10,...|  1.0|       0.0|\n",
      "|(100,[0,1,2,3,4,7...|  1.0|       1.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[1,4,5,6,9,1...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[4,5,7,9,10,...|  0.0|       0.0|\n",
      "|(100,[0,1,3,4,7,1...|  0.0|       0.0|\n",
      "|(100,[3,7,9,12,15...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[0,1,5,6,7,9...|  0.0|       0.0|\n",
      "|(100,[0,1,3,4,5,7...|  0.0|       0.0|\n",
      "|(100,[0,1,4,5,6,7...|  1.0|       1.0|\n",
      "|(100,[0,1,4,5,7,8...|  1.0|       1.0|\n",
      "|(100,[0,1,2,3,4,5...|  1.0|       1.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,6...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[0,2,3,4,5,7...|  0.0|       0.0|\n",
      "|(100,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(100,[1,5,9,10,11...|  0.0|       0.0|\n",
      "|(100,[1,3,4,10,13...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(te)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the best parameters for Logistic Regression on the test set using the Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We now test the best parameters through the pipeline. We first copy the evaluation metrics and output the minimum and maximum parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='HashingTF_47adb92303a5e769dd2a', name='numFeatures', doc='number of features.'): 100, Param(parent='LogisticRegression_472889e41ab9172ffc8f', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_472889e41ab9172ffc8f', name='regParam', doc='regularization parameter (>= 0).'): 0}\n",
      "{Param(parent='HashingTF_47adb92303a5e769dd2a', name='numFeatures', doc='number of features.'): 100, Param(parent='LogisticRegression_472889e41ab9172ffc8f', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_472889e41ab9172ffc8f', name='regParam', doc='regularization parameter (>= 0).'): 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "evaluation = model.validationMetrics.copy()\n",
    "minParams = np.argmin(evaluation)\n",
    "print(paramGrid[minParams])\n",
    "maxParams = np.argmax(evaluation)\n",
    "print(paramGrid[maxParams])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best testing model pipeline results:  0.9993894993894994\n",
      "Time taken is 31.141541242599487\n"
     ]
    }
   ],
   "source": [
    "# we input the best parameters from the grid search into the pipeline\n",
    "te_Start_time = time.time() #start timer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\",numFeatures=100)\n",
    "logRes = LogisticRegression(maxIter=10,regParam=0) #ml algo\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, logRes])  #pipeline\n",
    "model = pipeline.fit(te) #use pipeline to fit model\n",
    "prediction = model.transform(te) \n",
    "binary_class_eval=BinaryClassificationEvaluator()\n",
    "print(\"best testing model pipeline results: \", binary_class_eval.evaluate(prediction))\n",
    "te_End_time = time.time() #end timer\n",
    "te_TimeTaken = te_End_time - te_Start_time\n",
    "print(\"Time taken is\", te_TimeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the training time is much longer, however it also includes the tokenizer being initialised and the set up of the pipeline (not timing from the grid search). The accuracy is extremely high at near 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we test this on Naive Bayes and compare the results. As mentioned earlier, Naive Bayes is a popular algorithm, typcially used for this classification process. We implement the multinomial Naive Bayes model as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we train the naive bayes model, changing some of the variable names to avoid confusion with the logistic regression variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training model is  PipelineModel_4f1fb9d88f53f2dea8e4\n",
      "best training model results:  0.4904551356283106\n",
      "Time taken is 17.717833042144775\n"
     ]
    }
   ],
   "source": [
    "# Train a naive Bayes model.\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, hashingTF, nb]) \n",
    "\n",
    "#change paramgrid to add the the naive bayes smoothing parameter.\n",
    "nb_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100]) \\\n",
    "    .addGrid(nb.smoothing, [1.0, 0.5, 0]) \\\n",
    "    .build()\n",
    "\n",
    "#feed in the new naive bayes pipeline and grid search in TrainValidationSplit\n",
    "nb_train_val_split = TrainValidationSplit(estimator= nb_pipeline,\n",
    "                           estimatorParamMaps= nb_paramGrid,\n",
    "                           evaluator=binary_class_eval,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "#define the naive bayes model and fit to the training set\n",
    "nb_model = nb_train_val_split.fit(tr)\n",
    "\n",
    "nb_tr_Start_time = time.time() #start timer\n",
    "tr_pr_nb = nb_model.transform(tr) #nb prediction on training set\n",
    "print(\"The best training model is \", nb_model.bestModel)\n",
    "print(\"best training model results: \", binary_class_eval.evaluate(tr_pr_nb))\n",
    "nb_tr_End_time = time.time() #end timer\n",
    "nb_tr_TimeTaken = nb_tr_End_time - nb_tr_Start_time\n",
    "print(\"Time taken is\", nb_tr_TimeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we investigate the test set for Naive Bayes via modelling through TrainValidationSplit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best testing model is  PipelineModel_4f1fb9d88f53f2dea8e4\n",
      "best testing model results:  0.4674506779769934\n",
      "Time taken is 17.51353645324707\n"
     ]
    }
   ],
   "source": [
    "nb_te_Start_time = time.time() #start timer \n",
    "te_pr_nb = nb_model.transform(te) #nb prediction on testing set\n",
    "print(\"The best testing model is \", nb_model.bestModel)\n",
    "print(\"best testing model results: \", binary_class_eval.evaluate(te_pr_nb))\n",
    "nb_te_End_time = time.time() #end timer\n",
    "nb_te_TimeTaken = nb_te_End_time - nb_te_Start_time\n",
    "print(\"Time taken is\", nb_te_TimeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the best parameters for Naive Bayes on the test set using the Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We actually take the best parameters from the training set and apply these onto the test set via the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='HashingTF_4114b145928cb2fd5112', name='numFeatures', doc='number of features.'): 100, Param(parent='NaiveBayes_4dbca88328aa1e4680bd', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 1.0}\n",
      "{Param(parent='HashingTF_4114b145928cb2fd5112', name='numFeatures', doc='number of features.'): 100, Param(parent='NaiveBayes_4dbca88328aa1e4680bd', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "evaluation = nb_model.validationMetrics.copy()\n",
    "minParams = np.argmin(evaluation)\n",
    "print(nb_paramGrid[minParams])\n",
    "maxParams = np.argmax(evaluation)\n",
    "print(nb_paramGrid[maxParams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we take the best parameters from the grid search and apply it through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best testing model results:  0.46928217980849546\n",
      "Time taken is 31.52148461341858\n"
     ]
    }
   ],
   "source": [
    "nb_te_Start_time = time.time() #start timer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\",numFeatures=100)\n",
    "nb = NaiveBayes(smoothing= 0, modelType=\"multinomial\") #ML algo\n",
    "\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, hashingTF, nb]) \n",
    "nb_model = nb_pipeline.fit(te) #use pipeline to fit the model\n",
    "\n",
    "te_pr_nb = nb_model.transform(te) #nb prediction on testing set\n",
    "\n",
    "print(\"best testing model results: \", binary_class_eval.evaluate(te_pr_nb))\n",
    "nb_te_End_time = time.time() #end timer\n",
    "nb_te_TimeTaken = nb_te_End_time - nb_te_Start_time\n",
    "print(\"Time taken is\", nb_te_TimeTaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Naive Bayes is quite surprising as it performs poorly, below 50%. The time taken is longer however, it includes initialising the pipeline. The time is similar to that of Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Comparison of both models and analysis of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Surprisingly, the Naive Bayes models do not do as well as the logistic regression models. Both the training and testing accuracy for the best models were less than 50%. This in comparison to the high accuracy of near 100% given by the logistic regression models suggest that naive bayes was poor in this pipeline. Perhaps the limited amount of parameter changes resulted in such poor results. Naive Bayes is known for its limited parameter changes and its  simplicity. Perhaps the model initiated was too simple where in reality an ensemble or more sophisticated methods are employed. One thing to note is that there is a negligible difference in training and testing times for both machine learning algorithms.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
